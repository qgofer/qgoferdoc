{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Implementation of RAKE algorithm (Rapid Automatic Keyword Extraction\n",
    "  algorithm) based on https://github.com/u-prashant/RAKE\n",
    "output-file: rake.html\n",
    "title: rake\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%run 00_core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import operator\n",
    "import re\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "# Not the most elegant solution but works with the nbdev workflow\n",
    "try:\n",
    "    from qgoferdoc.core import stop_words\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_stop_words(stop_words_file_path: str) -> list:\n",
    "    \"\"\"Loads stop words from a file and return as a list of words.\n",
    "\n",
    "    Args:\n",
    "        stop_words_file_path:\n",
    "            filepath of a file containing stop words\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    file = open(stop_words_file_path)\n",
    "    for line in file:\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    file.close()\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def build_stop_word_regex(stop_words_file_path: str) -> re.Pattern:\n",
    "    \"\"\"Builds a regex expression to match any of the stop word.\n",
    "\n",
    "    Args:\n",
    "        stop_words_file_path:\n",
    "            filepath of a file containing stop words\n",
    "    \"\"\"\n",
    "    if stop_words_file_path:\n",
    "        stop_words_list = load_stop_words(stop_words_file_path)\n",
    "    else:\n",
    "        stop_words_list = stop_words\n",
    "    stop_words_regex_list = []\n",
    "    for word in stop_words_list:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'\n",
    "        stop_words_regex_list.append(word_regex)\n",
    "    stop_words_pattern = re.compile('|'.join(stop_words_regex_list), re.IGNORECASE)\n",
    "\n",
    "    return stop_words_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RAKE:\n",
    "    \"\"\"Rapid Automatic Keyword Extraction algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_words_file: str = ''):\n",
    "        \"\"\"Initialize the RAKE object.\"\"\"\n",
    "        self.stop_words_file_path = stop_words_file\n",
    "        self.stop_words_pattern = build_stop_word_regex(stop_words_file)\n",
    "\n",
    "    def exec(self, text: str):\n",
    "        \"\"\"Execute the RAKE algorithm.\"\"\"\n",
    "        sentences = self.split_sentences(text)\n",
    "        phrases = self.generate_candidate_keywords(sentences)\n",
    "        word_scores = self.calculate_word_scores(phrases)\n",
    "        keyword_candidates = self.generate_candidate_keyword_scores(phrases, word_scores)\n",
    "        sorted_keywords = sorted(keyword_candidates.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        return sorted_keywords\n",
    "\n",
    "    def split_sentences(self, text: str) -> list:\n",
    "        \"\"\"Split text into sentences.\"\"\"\n",
    "        pattern = u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s'\n",
    "        sentence_delimiters = re.compile(pattern)\n",
    "        sentences = sentence_delimiters.split(text)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def generate_candidate_keywords(self, sentences: list) -> list:\n",
    "        \"\"\"Returns keyword phrases after removing stopwords from each sentence.\"\"\"\n",
    "        phrases_list = []\n",
    "        for sentence in sentences:\n",
    "            phrases = re.sub(self.stop_words_pattern, '|', sentence.strip()).split('|')\n",
    "            for phrase in phrases:\n",
    "                phrase = phrase.strip().lower()\n",
    "                if phrase != \"\":\n",
    "                    phrases_list.append(phrase)\n",
    "\n",
    "        return phrases_list\n",
    "\n",
    "    def is_number(self, s):\n",
    "        \"\"\"Check if a string is a number.\"\"\"\n",
    "        try:\n",
    "            float(s) if '.' in s else int(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def separate_words(self, text: str, word_min_size: int = 0) -> list:\n",
    "        \"\"\"Return a list of all words of length greater than specified min size.\n",
    "\n",
    "        Args:\n",
    "            text:\n",
    "                the text that is to be split into words\n",
    "            word_min_size:\n",
    "                the min. no. of characters a word must have (def: 0)\n",
    "        \"\"\"\n",
    "        splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "        words = []\n",
    "        for single_word in splitter.split(text):\n",
    "            current_word = single_word.strip().lower()\n",
    "            if len(current_word) > word_min_size and current_word != '' and not self.is_number(current_word):\n",
    "                words.append(current_word)\n",
    "\n",
    "        return words\n",
    "\n",
    "    def calculate_word_scores(self, phrases: list) -> dict:\n",
    "        \"\"\"Calculates the word score for all the words in the phrases.\"\"\"\n",
    "        word_frequency: Dict[Any, Any] = {}\n",
    "        word_degree: Dict[Any, Any] = {}\n",
    "        for phrase in phrases:\n",
    "            words = self.separate_words(phrase)\n",
    "            words_list_degree = len(words) - 1\n",
    "            for word in words:\n",
    "                word_frequency.setdefault(word, 0)\n",
    "                word_frequency[word] += 1\n",
    "                word_degree.setdefault(word, 0)\n",
    "                word_degree[word] += words_list_degree\n",
    "\n",
    "        for item in word_frequency:\n",
    "            word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "        # Calculate word score = def(w) / freq(w)\n",
    "        word_score = {}\n",
    "        for item in word_frequency:\n",
    "            # word_score.setdefault(item, 0):\n",
    "            word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)\n",
    "\n",
    "        return word_score\n",
    "\n",
    "    def generate_candidate_keyword_scores(self, phrases: list, word_score: dict) -> dict:\n",
    "        \"\"\"Returns the dict. of candidate keywords with scores.\"\"\"\n",
    "        keyword_candidates: Dict[Any, Any] = {}\n",
    "        for phrase in phrases:\n",
    "            keyword_candidates.setdefault(phrase, 0)\n",
    "            words = self.separate_words(phrase)\n",
    "            candidate_score = 0\n",
    "            for word in words:\n",
    "                candidate_score += word_score[word]\n",
    "            keyword_candidates[phrase] = candidate_score\n",
    "\n",
    "        return keyword_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
